---
title: "Bring Your Own LLM"
description: "Connect your own AI models to BrowserOS"
---

BrowserOS includes a default AI model you can use right away, but it has strict rate limits. For the best experience, bring your own API keys or run models locally.

## Which Model Should I Use?

| Mode | What works | Recommendation |
|------|------------|----------------|
| **Chat Mode** | Any model, including local | Ollama or Gemini Flash |
| **Agent Mode** | Cloud models only | Claude Opus 4.5 or Kimi K2.5 (open source) |

<Warning>
**Local LLMs aren't powerful for most agentic tasks yet.** They're great for Chat — asking questions about a page, summarizing, etc. But agent tasks need strong reasoning to click the right elements and handle multi-step workflows. Use Claude Opus 4.5, Sonnet 4.5, or Kimi K2.5 for agents.
</Warning>

<Note>
Kimi K2.5 is an open-source, multimodal model with great agentic performance — and 60-70% cheaper than Claude models.
</Note>

---

## Cloud Providers

Connect to powerful AI models using your API keys. Your keys stay on your machine — requests go directly to the provider.

<AccordionGroup>
  <div id="gemini" />
  <Accordion title="Gemini (Free)" icon="google">
    Gemini Flash is fast and free. Google gives you 20 requests per minute at no cost.

    **Get your API key:**
    1. Go to [aistudio.google.com](https://aistudio.google.com)
    2. Click **Get API key** in the sidebar
    3. Click **Create API key** and copy it

    ![Get Gemini API key](/images/gemini-get-api-key.png)

    **Add to BrowserOS:**
    1. Go to `chrome://browseros/settings`
    2. Click **USE** on the Gemini card
    3. Set **Model ID** to `gemini-2.5-flash-preview-05-20`
    4. Paste your API key
    5. Check **Supports Images**, set **Context Window** to `1000000`
    6. Click **Save**

    ![Gemini config](/images/byollm--gemini-provider-config.png)
  </Accordion>

  <div id="claude" />
  <Accordion title="Claude (Best for Agents)" icon="message-bot">
    Claude Opus 4.5 gives the best results for Agent Mode.

    **Get your API key:**
    1. Go to [console.anthropic.com](https://console.anthropic.com/dashboard)
    2. Click **API keys** in the sidebar
    3. Click **Create Key** and copy it

    ![Get Claude API key](/images/claude-api-keys.png)

    **Add to BrowserOS:**
    1. Go to `chrome://browseros/settings`
    2. Click **USE** on the Anthropic card
    3. Set **Model ID** to `claude-opus-4-5-20250514`
    4. Paste your API key
    5. Check **Supports Images**, set **Context Window** to `200000`
    6. Click **Save**

    ![Claude config](/images/byollm--claude-provider-config.png)
  </Accordion>

  <div id="openai" />
  <Accordion title="OpenAI" icon="brain">
    GPT-4.1 is solid for both chat and agent tasks.

    **Get your API key:**
    1. Go to [platform.openai.com](https://platform.openai.com)
    2. Click settings icon → **API keys**
    3. Click **Create new secret key** and copy it

    ![Get OpenAI API key](/images/openai-api-keys.png)

    **Add to BrowserOS:**
    1. Go to `chrome://browseros/settings`
    2. Click **USE** on the OpenAI card
    3. Set **Model ID** to `gpt-4.1`
    4. Paste your API key
    5. Check **Supports Images**, set **Context Window** to `128000`
    6. Click **Save**

    ![OpenAI config](/images/byollm--openai-provider-config.png)
  </Accordion>

  <div id="openrouter" />
  <Accordion title="OpenRouter" icon="shuffle">
    Access 500+ models through one API.

    **Get your API key:**
    1. Go to [openrouter.ai](https://openrouter.ai) and sign up
    2. Copy your API key from the homepage

    **Pick a model:**
    Go to [openrouter.ai/models](https://openrouter.ai/models) and copy the model ID you want (e.g., `anthropic/claude-opus-4.5`).

    ![OpenRouter models](/images/openrouter-models.png)

    **Add to BrowserOS:**
    1. Go to `chrome://browseros/settings`
    2. Click **USE** on the OpenRouter card
    3. Paste the model ID and your API key
    4. Set **Context Window** based on the model
    5. Click **Save**

    ![OpenRouter config](/images/byollm--openrouter-provider-config.png)
  </Accordion>
</AccordionGroup>

---

## Local Models

<Card title="Local Model Guide" icon="server" href="/features/local-models">
  Run AI completely offline with Ollama or LM Studio. Includes recommended models, context length setup, and configuration steps.
</Card>

---

## Switching Between Models

Use the model switcher in the Assistant panel to change providers anytime. The default provider is highlighted.

![Model switcher](/images/byollm--switcher.png)

<Tip>
Use local models for sensitive work data. Switch to Claude for agent tasks that need complex reasoning.
</Tip>
